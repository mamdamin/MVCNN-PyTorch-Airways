{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/apps/tensorflow/1.7.0_cuda-9.0.176.1_python-3.6.4/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import tensorflow as tf\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL.Image import BILINEAR\n",
    "from multiprocessing import Process, freeze_support, set_start_method\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import uuid\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from models.resnet import *\n",
    "from augment import augmentImages\n",
    "\n",
    "import util\n",
    "from logger import Logger\n",
    "from custom_dataset import MultiViewDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='MVCNN-PyTorch')\n",
    "parser.add_argument('--data', metavar='DIR', default='/localscratch/Users/amotahari/MV_CNN_views', help='path to dataset')\n",
    "parser.add_argument('--resnet', default=18, choices=[18, 34, 50, 101, 152], type=int, metavar='N', help='resnet depth (default: resnet18)')\n",
    "parser.add_argument('--epochs', default=100, type=int, metavar='N', help='number of total epochs to run (default: 100)')\n",
    "parser.add_argument('-b', '--batch-size', default=12, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 4)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.01, type=float,\n",
    "                    metavar='LR', help='initial learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum (default: 0.9)')\n",
    "parser.add_argument('--lr-decay-freq', default=30, type=float,\n",
    "                    metavar='W', help='learning rate decay (default: 30)')\n",
    "parser.add_argument('--lr-decay', default=0.1, type=float,\n",
    "                    metavar='W', help='learning rate decay (default: 0.1)')\n",
    "parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                    metavar='N', help='print frequency (default: 10)')\n",
    "parser.add_argument('-r', '--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('-o', '--output', default='/Shared/CTmechanics_COPDGene/Amin/Airway_PyTorch', type=str, metavar='PATH',\n",
    "                    help='path to Output folder for logs and checkpoints (default: none)')\n",
    "parser.add_argument('-w', '--workers', default=0, type=int,\n",
    "                    metavar='N', help='Number of workers in input pipr (default: 4)')\n",
    "parser.add_argument('-f', '--fun', default='', type=str, metavar='PATH',\n",
    "                    help='path to Output folder for logs and checkpoints (default: none)')\n",
    "#parser.add_argument('--pretrained', dest='pretrained', action='store_true', help='use pre-trained model')\n",
    "\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "/localscratch/Users/amotahari/MV_CNN_views/sets/train.txt\n",
      "Loading train data: 100% \n",
      "Training Data Loaded!\n",
      "/localscratch/Users/amotahari/MV_CNN_views/sets/validation.txt\n",
      "Loading validation data: 100% \n",
      "Validation Data Loaded!\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(500),\n",
    "    #transforms.RandomAffine(30, translate=(.2,.2), scale=None, shear=None, resample=BILINEAR, fillcolor=0), # Augmentation\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#resnet = nn.DataParallel(resnet18(num_classes=2), device_ids=[0, 1, 2, 3])\n",
    "\n",
    "# Load dataset\n",
    "#try:\n",
    "#    fileObject = open(os.path.join(args.data,'trainingDataset.HD5'),'r')\n",
    "#    dset_train = pickle.load(fileObject)\n",
    "#except: \n",
    "\n",
    "print('Loading data')\n",
    "dset_train = MultiViewDataSet(args.data, 'train', transform=transform)\n",
    "#fileObject = open(os.path.join(args.data,'trainingDataset.HD5'),'wb')\n",
    "#pickle.dump(dset_train,fileObject)\n",
    "print(\"\\nTraining Data Loaded!\")\n",
    "\n",
    "#try:\n",
    "#    fileObject = open(os.path.join(args.data,'validationDataset.HD5'),'r')\n",
    "#    dset_val = pickle.load(fileObject)\n",
    "#except: \n",
    "dset_val = MultiViewDataSet(args.data, 'validation', transform=transform)\n",
    "#    fileObject = open(os.path.join(args.data,'validationDataset.HD5'),'wb')\n",
    "#    pickle.dump(dset_train,fileObject)\n",
    "print(\"\\nValidation Data Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(dset_val, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)\n",
    "train_loader = DataLoader(dset_train, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ['Standard', 'Abnormal']\n",
      "Using resnet18\n",
      "CUDA devices available:  range(0, 4)\n",
      "Running on cuda:0\n",
      "\n",
      "-----------------------------------\n",
      "Epoch: [1/100]\n",
      "\tIter [10/121] Loss: 1.6790\n",
      "\tIter [20/121] Loss: 0.7865\n",
      "\tIter [30/121] Loss: 0.6794\n",
      "\tIter [40/121] Loss: 0.5776\n",
      "\tIter [50/121] Loss: 1.0442\n",
      "\tIter [60/121] Loss: 0.6347\n",
      "\tIter [70/121] Loss: 0.7601\n",
      "\tIter [80/121] Loss: 0.5016\n",
      "\tIter [90/121] Loss: 0.6640\n",
      "\tIter [100/121] Loss: 1.0641\n",
      "\tIter [110/121] Loss: 0.7914\n",
      "\tIter [120/121] Loss: 0.7423\n",
      "Time taken: 189.30 sec.\n",
      "\n",
      "Evaluation:\n",
      "\tVal Acc: 36.00 - Loss: 0.7846\n",
      "\tCurrent best val acc: 0.00\n",
      "\tSaving checkpoint - Acc: 36.00\n",
      "\n",
      "-----------------------------------\n",
      "Epoch: [2/100]\n",
      "\tIter [10/121] Loss: 0.4938\n",
      "\tIter [20/121] Loss: 0.6850\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-88b245442e71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time taken: %.2f sec.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-88b245442e71>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# compute gradient and do SGD step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/pytorch/0.4.0_cuda-9.0.176.1_python-3.6.4/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/pytorch/0.4.0_cuda-9.0.176.1_python-3.6.4/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "args.output = os.path.join(args.output, str(uuid.uuid4().hex))\n",
    "if not os.path.exists(args.output):\n",
    "    os.makedirs(args.output)\n",
    "\n",
    "with open(os.path.join(args.output,'Arguments.txt'), \"w\") as text_file:\n",
    "    print(args, file = text_file) #text_file.write(args)\n",
    "    \n",
    "classes = dset_train.classes\n",
    "print(len(classes), classes)\n",
    "\n",
    "if args.resnet == 18:\n",
    "    resnet = resnet18(num_classes=len(classes))\n",
    "elif args.resnet == 34:\n",
    "    resnet = resnet34(num_classes=len(classes))\n",
    "elif args.resnet == 50:\n",
    "    resnet = resnet50(num_classes=len(classes))\n",
    "elif args.resnet == 101:\n",
    "    resnet = resnet101(num_classes=len(classes))\n",
    "elif args.resnet == 152:\n",
    "    resnet = resnet152(num_classes=len(classes))\n",
    "\n",
    "print('Using resnet' + str(args.resnet))\n",
    "resnet.to(device)\n",
    "device_ids = range(torch.cuda.device_count())\n",
    "print(\"CUDA devices available: \",device_ids)\n",
    "resnet = nn.DataParallel(resnet, device_ids=[0,1,2])\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print('Running on ' + str(device))\n",
    "\n",
    "logger = Logger(os.path.join(args.output, 'logs'))\n",
    "\n",
    "# Loss and Optimizer\n",
    "lr = args.lr\n",
    "n_epochs = args.epochs\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=lr)\n",
    "\n",
    "best_acc = 0.0\n",
    "best_loss = 0.0\n",
    "start_epoch = 0\n",
    "\n",
    "# Helper functions\n",
    "def load_checkpoint():\n",
    "    global best_acc, start_epoch\n",
    "    # Load checkpoint.\n",
    "    print('\\n==> Loading checkpoint..')\n",
    "    assert os.path.isfile(args.resume), 'Error: no checkpoint file found!'\n",
    "\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    resnet.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "\n",
    "def train():\n",
    "    train_size = len(train_loader)\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Convert from list of 3D to 4D\n",
    "        #inputs = np.stack(inputs, axis=1)\n",
    "        inputs = np.stack(inputs, axis=0)\n",
    "        #print(\"shape of Train input= \", inputs.shape)        \n",
    "        #inputs = augment_on_GPU(inputs)\n",
    "        inputs = torch.from_numpy(inputs)\n",
    "        #print(\"shape of Train input from numpy= \", inputs.shape)  \n",
    "        inputs, targets = inputs.cuda(), targets.cuda(0)\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        \n",
    "        # compute output\n",
    "        outputs = resnet(inputs)\n",
    "        #print(outputs.get_device(), targets.get_device())\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % args.print_freq == 0:\n",
    "            print(\"\\tIter [%d/%d] Loss: %.4f\" % (i + 1, train_size, loss.item()))\n",
    "\n",
    "\n",
    "# Validation and Testing\n",
    "def eval(data_loader, is_test=False):\n",
    "    if is_test:\n",
    "        load_checkpoint()\n",
    "\n",
    "    # Eval\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            # Convert from list of 3D to 4D\n",
    "            inputs = np.stack(inputs, axis=0)\n",
    "            #print(\"shape of Val input= \", inputs.shape)\n",
    "            #inputs = augment_on_GPU(inputs)\n",
    "            inputs = torch.from_numpy(inputs)\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda(0)\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "\n",
    "            # compute output\n",
    "            outputs = resnet(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss\n",
    "            n += 1\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted.cpu() == targets.cpu()).sum()\n",
    "\n",
    "    avg_test_acc = 100 * correct / total\n",
    "    avg_loss = total_loss / n\n",
    "\n",
    "    return avg_test_acc, avg_loss\n",
    "\n",
    "# Training / Eval loop\n",
    "if args.resume:\n",
    "    load_checkpoint()\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    print('\\n-----------------------------------')\n",
    "    print('Epoch: [%d/%d]' % (epoch+1, n_epochs))\n",
    "    start = time.time()\n",
    "\n",
    "    resnet.train()\n",
    "    train()\n",
    "    print('Time taken: %.2f sec.' % (time.time() - start))\n",
    "\n",
    "    resnet.eval()\n",
    "    avg_test_acc, avg_loss = eval(val_loader)\n",
    "\n",
    "    print('\\nEvaluation:')\n",
    "    print('\\tVal Acc: %.2f - Loss: %.4f' % (avg_test_acc.item(), avg_loss.item()))\n",
    "    print('\\tCurrent best val acc: %.2f' % best_acc)\n",
    "\n",
    "    # Log epoch to tensorboard\n",
    "    # See log using: tensorboard --logdir='logs' --port=6006\n",
    "    util.logEpoch(logger, resnet, epoch + 1, avg_loss, avg_test_acc)\n",
    "\n",
    "    # Save model\n",
    "    if avg_test_acc > best_acc:\n",
    "        print('\\tSaving checkpoint - Acc: %.2f' % avg_test_acc)\n",
    "        best_acc = avg_test_acc\n",
    "        best_loss = avg_loss\n",
    "        util.save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': resnet.state_dict(),\n",
    "            'acc': avg_test_acc,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            },\n",
    "            checkpoint = args.output\n",
    "        )\n",
    "\n",
    "    # Decaying Learning Rate\n",
    "    if (epoch + 1) % args.lr_decay_freq == 0:\n",
    "        lr *= args.lr_decay\n",
    "        optimizer = torch.optim.Adam(resnet.parameters(), lr=lr)\n",
    "        print('Learning rate:', lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape:  torch.Size([12, 48, 3, 224, 224])\n",
      "Reading batch 120 of 121 \n",
      "Time to read one epoch:  74.59 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Check input pipeline throughput\n",
    "start = time.time()\n",
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "    if i==0:\n",
    "        print(\"Input tensor shape: \",inputs.shape)\n",
    "    sys.stdout.write('Reading batch {} of {} \\r'.format(i+1,len(train_loader)))\n",
    "    sys.stdout.flush()\n",
    "#    if i==10:\n",
    "#        break\n",
    "print('\\nTime to read one epoch:  %.2f seconds.' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
