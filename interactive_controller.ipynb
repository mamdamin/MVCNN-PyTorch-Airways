{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL.Image import BILINEAR\n",
    "from multiprocessing import Process, freeze_support, set_start_method\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import uuid\n",
    "import os\n",
    "import sys\n",
    "from importlib import reload\n",
    "from models.resnet import *\n",
    "import augment\n",
    "import util\n",
    "from logger import Logger\n",
    "from custom_dataset import MultiViewDataSet\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "!~/copydata2local.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='MVCNN-PyTorch')\n",
    "parser.add_argument('--data', metavar='DIR', default='/localscratch/Users/amotahari/MV_CNN_views', help='path to dataset')\n",
    "parser.add_argument('-j', '--job_id', metavar='ID', default='', help='SGE job ID')\n",
    "parser.add_argument('--resnet', default=18, choices=[18, 34, 50, 101, 152], type=int, metavar='N', help='resnet depth (default: resnet18)')\n",
    "parser.add_argument('--epochs', default=10000, type=int, metavar='N', help='number of total epochs to run (default: 100)')\n",
    "parser.add_argument('-b', '--batch-size', default=24, type=int, metavar='N', help='mini-batch size (default: 4)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.00001, type=float, metavar='LR', help='initial learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum (default: 0.9)')\n",
    "parser.add_argument('--lr-decay-freq', default=200, type=float, metavar='W', help='learning rate decay (default: 30)')\n",
    "parser.add_argument('--lr-decay', default=1.0, type=float, metavar='W', help='learning rate decay (default: 0.1)')\n",
    "parser.add_argument('--print-freq', '-p', default=10, type=int, metavar='N', help='print frequency (default: 10)')\n",
    "parser.add_argument('-r', '--resume', default='/Shared/CTmechanics_COPDGene/Amin/Airway_PyTorch/607310/checkpoint.pth.tar', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('-o', '--output', default='/Shared/CTmechanics_COPDGene/Amin/Airway_PyTorch', type=str, metavar='PATH',\n",
    "                    help='path to Output folder for logs and checkpoints (default: none)')\n",
    "parser.add_argument('-w', '--workers', default=0, type=int, metavar='N', help='Number of workers in input pipe (default: 4)')\n",
    "parser.add_argument('-wd', '--weight_decay', default=0.0, type=float, metavar='W', help='Weight decay factor (default: 0.1)')\n",
    "parser.add_argument('--l1weight', default=0.0, type=float, metavar='W', help='L1 Regularization Weight (default: 0.0)')\n",
    "parser.add_argument('--mode', default='train', type=str, metavar='M', help='Operating mode (default: train)')\n",
    "parser.add_argument('--view_step', default=1, type=int, metavar='N', help='Steps in selecting views (default: 1)')\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument('-f', '--fun', default='', type=str, metavar='PATH',\n",
    "                    help='path to Output folder for logs and checkpoints (default: none)')\n",
    "#parser.add_argument('--pretrained', dest='pretrained', action='store_true', help='use pre-trained model')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "'''\n",
    "args.mode = 'train'\n",
    "args.resume = ''\n",
    "args.l1weight = 10.0\n",
    "'''\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    #transforms.CenterCrop(500),\n",
    "    #transforms.RandomAffine(30, translate=(.2,.2), scale=None, shear=None, resample=BILINEAR, fillcolor=0), # Augmentation\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Loading data')\n",
    "if args.mode=='test':\n",
    "    dset_test = MultiViewDataSet(args.data, 'test', step = args.view_step, transform=transform)\n",
    "    nofviews = dset_test.nofviews\n",
    "    print(\"\\nTest Data Loaded!\")\n",
    "else:\n",
    "    dset_train = MultiViewDataSet(args.data, 'train', step = args.view_step, transform=transform)\n",
    "    print(\"\\nTraining Data Loaded!\")\n",
    "    dset_val = MultiViewDataSet(args.data, 'validation', step = args.view_step, transform=transform)\n",
    "    print(\"\\nValidation Data Loaded!\")\n",
    "    nofviews = dset_train.nofviews\n",
    "    \n",
    "print(\"Number of views per subject =\", nofviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.mode=='test':\n",
    "    test_loader = DataLoader(dset_test, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "    classes = dset_test.classes\n",
    "else:\n",
    "    val_loader = DataLoader(dset_val, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)\n",
    "    train_loader = DataLoader(dset_train, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)\n",
    "    classes = dset_train.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "\n",
    "if args.job_id =='':\n",
    "    job_folder = \"Jupyter_\"+time.asctime( time.localtime(time.time()) )#str(uuid.uuid4().hex)[:5]\n",
    "    job_folder = job_folder.replace(' ','_').replace(':','_')\n",
    "else:\n",
    "    job_folder = args.job_id\n",
    "\n",
    "print(job_folder)\n",
    "\n",
    "args.output = os.path.join(args.output, job_folder)\n",
    "if not os.path.exists(args.output):\n",
    "    os.makedirs(args.output)\n",
    "\n",
    "with open(os.path.join(args.output,'Arguments.txt'), \"w\") as text_file:\n",
    "    print(args, file = text_file) #text_file.write(args)\n",
    "    \n",
    "#classes = dset_train.classes\n",
    "\n",
    "print(len(classes), classes)\n",
    "\n",
    "if args.resnet == 18:\n",
    "    resnet = resnet18(num_classes=len(classes))\n",
    "elif args.resnet == 34:\n",
    "    resnet = resnet34(num_classes=len(classes))\n",
    "elif args.resnet == 50:\n",
    "    resnet = resnet50(num_classes=len(classes))\n",
    "elif args.resnet == 101:\n",
    "    resnet = resnet101(num_classes=len(classes))\n",
    "elif args.resnet == 152:\n",
    "    resnet = resnet152(num_classes=len(classes))\n",
    "\n",
    "print('Using resnet' + str(args.resnet))\n",
    "resnet.to(device)\n",
    "device_ids = range(torch.cuda.device_count())\n",
    "print(\"CUDA devices available: \",device_ids)\n",
    "resnet = nn.DataParallel(resnet, device_ids=device_ids)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print('Running on ' + str(device))\n",
    "\n",
    "logger = Logger(os.path.join(args.output, 'logs'))\n",
    "\n",
    "# Loss and Optimizer\n",
    "lr = args.lr\n",
    "n_epochs = args.epochs\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=lr, weight_decay=args.weight_decay)\n",
    "_,model = next(resnet.named_children())\n",
    "for name,fc_layer in model.named_children():\n",
    "    if name in ['fc']:\n",
    "        break\n",
    "\n",
    "best_acc = 0.0\n",
    "best_loss = 0.0\n",
    "start_epoch = 0\n",
    "\n",
    "# Helper functions\n",
    "def load_checkpoint():\n",
    "    global best_acc, start_epoch\n",
    "    # Load checkpoint.\n",
    "    print('\\n==> Loading checkpoint..')\n",
    "    assert os.path.isfile(args.resume), 'Error: no checkpoint file found!'\n",
    "\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    resnet.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "\n",
    "def train():\n",
    "    train_size = len(train_loader)\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Convert from list of 3D to 4D\n",
    "        #inputs = np.stack(inputs, axis=1)\n",
    "        #inputs = np.stack(inputs, axis=0)\n",
    "        inputs = augmentor.augment_on_GPU(inputs)\n",
    "\n",
    "        #print(\"shape of Train input= \", inputs.shape)        \n",
    "        #inputs = augment_on_GPU(inputs)\n",
    "        inputs = torch.from_numpy(inputs)\n",
    "        #print(\"shape of Train input from numpy= \", inputs.shape)  \n",
    "        inputs, targets = inputs.cuda(), targets.cuda(0)\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        \n",
    "        # compute output\n",
    "        outputs = resnet(inputs)\n",
    "        #print(outputs.get_device(), targets.get_device())\n",
    "        regularization_loss = args.l1weight*torch.mean(torch.abs(fc_layer.weight))\n",
    "                \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss += regularization_loss\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % args.print_freq == 0:\n",
    "            #print(regularization_loss)\n",
    "            print(\"\\tIter [%d/%d] Loss: %.4f, Reg: %.6f\" % (i + 1, train_size, loss.item(), regularization_loss.item()))\n",
    "\n",
    "\n",
    "# Validation and Testing\n",
    "def eval(data_loader, is_test=False):\n",
    "    if is_test:\n",
    "        load_checkpoint()\n",
    "\n",
    "    # Eval\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            # Convert from list of 3D to 4D\n",
    "            #inputs = np.stack(inputs, axis=0)\n",
    "            inputs = augmentor.augment_on_GPU(inputs)\n",
    "            \n",
    "            #print(\"shape of Val input= \", inputs.shape)\n",
    "            #inputs = augment_on_GPU(inputs)\n",
    "            inputs = torch.from_numpy(inputs)\n",
    "\n",
    "            inputs, targets = inputs.cuda(), targets.cuda(0)\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "\n",
    "            # compute output\n",
    "            outputs = resnet(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss\n",
    "            n += 1\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted.cpu() == targets.cpu()).sum()\n",
    "\n",
    "    avg_test_acc = 100 * correct / total\n",
    "    avg_loss = total_loss / n\n",
    "\n",
    "    return avg_test_acc, avg_loss\n",
    "\n",
    "def test(data_loader):\n",
    "    resnet.eval()\n",
    "    # Eval\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    test_output=[]\n",
    "    test_target=[]\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            # Convert from list of 3D to 4D\n",
    "            #inputs = np.stack(inputs, axis=0)\n",
    "            A = inputs\n",
    "            inputs = augmentor.augment_on_GPU(inputs)\n",
    "            #print(\"shape of Val input= \", inputs.shape)\n",
    "            #inputs = augment_on_GPU(inputs)\n",
    "            inputs = torch.from_numpy(inputs)\n",
    "            #test_target.append(targets.data.numpy())\n",
    "            test_target = targets.data.numpy()\n",
    "            inputs, targets = inputs.cuda(), targets.cuda(0)\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "\n",
    "            # compute output\n",
    "            outputs = resnet(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss\n",
    "            n += 1\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted.cpu() == targets.cpu()).sum()\n",
    "            \n",
    "            test_output.append(outputs.cpu().data.numpy())\n",
    "            \n",
    "            #print(test_target)\n",
    "            #print(test_output)\n",
    "            for i in range(args.batch_size):\n",
    "                if test_target[i] > 2:\n",
    "                    B = A[i,4,:].transpose(0, 2)\n",
    "                    I1 = B.transpose(0,1)\n",
    "                    I1 = (I1.data.numpy())\n",
    "                    I = inputs.cpu()\n",
    "                    B = I[i,6,:].transpose(0, 2)\n",
    "                    I2 = B.transpose(0,1)\n",
    "                    I2 = (I2.data.numpy()) \n",
    "\n",
    "                    #print(np.amin(I2-I1))\n",
    "                    #print(np.amax(I2-I1))\n",
    "                    plt.figure(figsize=(20, 40))\n",
    "                    plt.subplot(1,2,1)\n",
    "                    plt.imshow(1-I1)\n",
    "                    plt.subplot(1,2,2)\n",
    "                    plt.imshow(1-I2)\n",
    "            break\n",
    "            \n",
    "            \n",
    "    avg_test_acc = 100 * correct / total\n",
    "    avg_loss = total_loss / n\n",
    "\n",
    "    return avg_test_acc, avg_loss\n",
    "\n",
    "# Training / Eval loop\n",
    "reload(augment)\n",
    "augmentor = augment.augmentor(nofviews,ngpus = len(device_ids))\n",
    "\n",
    "if args.resume or args.mode == 'test':\n",
    "    load_checkpoint()\n",
    "\n",
    "    \n",
    "if args.mode == 'test':\n",
    "    print('Testing:')\n",
    "    avg_test_acc, avg_loss = test(test_loader)\n",
    "    print('\\tVal Acc: %.2f - Loss: %.4f' % (avg_test_acc.item(), avg_loss.item()))\n",
    "    print('\\tCurrent best val acc: %.2f' % best_acc)    \n",
    "    \n",
    "else:\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        print('\\n-----------------------------------')\n",
    "        print('Epoch: [%d/%d]' % (epoch+1, n_epochs))\n",
    "        start = time.time()\n",
    "\n",
    "        resnet.train()\n",
    "        train()\n",
    "        print('Time taken: %.2f sec.' % (time.time() - start))\n",
    "        if (epoch + 1) % 5 == 0: # Eval every 5 epoch\n",
    "            resnet.eval()\n",
    "            avg_test_acc, avg_loss = eval(val_loader)\n",
    "\n",
    "            print('\\nEvaluation:')\n",
    "            print('\\tVal Acc: %.2f - Loss: %.4f' % (avg_test_acc.item(), avg_loss.item()))\n",
    "            print('\\tCurrent best val acc: %.2f' % best_acc)\n",
    "\n",
    "            # Log epoch to tensorboard\n",
    "            # See log using: tensorboard --logdir='logs' --port=6006\n",
    "            util.logEpoch(logger, resnet, epoch + 1, avg_loss, avg_test_acc)\n",
    "\n",
    "            # Save model\n",
    "            if avg_test_acc > best_acc:\n",
    "                print('\\tSaving checkpoint - Acc: %.2f' % avg_test_acc)\n",
    "                best_acc = avg_test_acc\n",
    "                best_loss = avg_loss\n",
    "                util.save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': resnet.state_dict(),\n",
    "                    'acc': avg_test_acc,\n",
    "                    'best_acc': best_acc,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    },\n",
    "                    checkpoint = args.output\n",
    "                )\n",
    "\n",
    "        # Decaying Learning Rate\n",
    "        if (epoch + 1) % args.lr_decay_freq == 0:\n",
    "            lr *= args.lr_decay\n",
    "            optimizer = torch.optim.Adam(resnet.parameters(), lr=lr, weight_decay=args.weight_decay)\n",
    "            print('Learning rate:', lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check input pipeline throughput\n",
    "start = time.time()\n",
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "    #inputs = augmentor.augment_on_GPU(inputs)\n",
    "    if i==0:\n",
    "        print(\"Input tensor shape: \",inputs.shape)\n",
    "    \n",
    "    sys.stdout.write('Reading batch {} of {} \\r'.format(i+1,len(train_loader)))\n",
    "    sys.stdout.flush()\n",
    "#    if i==10:\n",
    "#        break\n",
    "print('\\nTime to read one epoch:  %.2f seconds.' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,model = next(resnet.named_children())\n",
    "for name,module in model.named_children():\n",
    "    if name in ['fc']:\n",
    "        print(module.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[0:2,:].shape)\n",
    "\n",
    "A = augment_on_GPU(inputs)\n",
    "A = torch.from_numpy(A)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[0,0,:].shape)\n",
    "\n",
    "C = A-inputs\n",
    "B = C[0,5,:].transpose(0, 2)\n",
    "B = B.transpose(0,1)\n",
    "plt.imshow(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script interactive_controller.ipynb --output controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "A = str(subprocess.check_output([\"{job_id}\"]))\n",
    "print(\"s = \",A)\n",
    "#print(subprocess.call(\"nvidia-smi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
